\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}

\begin{document}

\tableofcontents

\pagebreak

\section{Introduction to Machine Learning}

\begin{itemize}
    \item Machine learning algorithms are statistical algorithms that can learn from data and generalize to unseen data, thus performing tasks without explicit instructions.
    \item In supervised methods we know the outcome Y for each input X; often, the task is to predict Y based on X.
    \item In unsupervised methods we have only the inputs X. Typical tasks are about finding structure in data.
    \begin{itemize}
        \item For example, it can be used to find "clusters".
    \end{itemize}
    \item X is often known as:
    \begin{itemize}
        \item Input
        \item Feature
        \item Predictor
        \item Covariate
        \item Independent variable
    \end{itemize}
    \item Y is often known as:
    \begin{itemize}
        \item Output
        \item Outcome
        \item Response
        \item Target variable
        \item Dependent variable
    \end{itemize}
    \item Supervised problems are often referred to as either:
    \begin{itemize}
        \item \textbf{Regression}: The outcome Y is quantitative (typically $\mathbb{R}$)s
        \item \textbf{Classification}: The outcome Y is categorical
    \end{itemize}
\end{itemize}

\pagebreak

\section{Regression}

\subsection{Regression Setting}

Assume a functional relationship between $X$ and $Y$ as

\begin{align*}
    Y=f(X)+\varepsilon
\end{align*}

The noise $\varepsilon$:
\begin{itemize}
    \item Has mean zero
    \item Has constant variance
    \item Is generally uncorrelated between observations
\end{itemize}

Formulation in terms of the conditional mean of $Y$ given features $X$:

\begin{align*}
    \mathbb{E}(Y|X)=f(X)
\end{align*}

A natural prediction $\hat{Y}$ for a new observation with input $X_0$ is:

\begin{align*}
    \hat{Y}=\mathbb{E}(Y|X_0)=f(X_0)
\end{align*}

\subsection{Linear regression - general formulation}

For $p$ features and an intercept term the model is:

\begin{align*}
    \mathbf{Y}=\mathbf{X}\beta+\varepsilon
\end{align*}
\begin{align*}
    \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_n
    \end{bmatrix}
    =
    \begin{bmatrix}
    1 & x_{i1} & \hdots & x_{ip} \\
    \vdots & \vdots & \vdots & \vdots \\
    1 & x_{i1} & \hdots & x_{ip} \\
    \end{bmatrix}
    \begin{bmatrix}
    \beta_1 \\
    \vdots \\
    \beta_p
    \end{bmatrix}
    +
    \begin{bmatrix}
    \varepsilon_1 \\
    \vdots \\
    \varepsilon_n
    \end{bmatrix}
\end{align*}

\subsection{Categorical features}

When a variable is not continuous, \textit{dummy variables} are used, referred to as \textit{one-hot encoding} in ML

Example, for a binary variable:

\begin{align*}
    Y=\beta_0+\beta_1\mathbbm{1}_{X=B}+\varepsilon
\end{align*}

\begin{itemize}
    \item $\beta_0$ is the mean in group $A$
    \item $\beta_1$ is the difference in group means between groups $B$ and $A$
\end{itemize}

\pagebreak

\subsection{Complex functional relationships}

When $Y$ has nonlinear term, for example: $x_1x_2$
We can define $x_3=x_1x_2$ to obtain:

\begin{align*}
    Y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\epsilon
\end{align*}


\subsection{Estimating parameters $\beta$ and $\sigma_2$}

It's advisable to make scatter plots first, before estimating parameters.

\subsubsection{RSS}

To estimate $\beta$, the least squares method is used - it finds $\beta$ that minimises the sum of squared errors/residuals.

\begin{align*}
    RSS = \sum_{i=1}^n (y_i-\hat{y})^2=\sum_{i=1}^n(y_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2
\end{align*}

In closed matrix form, the solution to find $\hat{\beta}$ is:

\begin{align*}
    \hat{\beta}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}
\end{align*}

And thus:

\begin{align*}
    \hat{\boldsymbol{Y}}=\boldsymbol{X}\hat{\beta}=\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}
\end{align*}

Note: if $p\geq n$, this won't work.

\subsubsection{MLE}

For an individual $y_i$ it is:

\begin{align*}
    p_Y(y_i|X)=\frac{1}{(2\pi\sigma^2)^{1/2}}
    \cdot
    e^{-\frac{1}{2}\frac{(y_i-x_i^T\beta)^2}{\sigma^2}}
\end{align*}

For all $y_i$ it is:

\begin{align*}
    \prod_{i=1}^n p_Y(y_i|X)
\end{align*}

To simplify the working, the log-likelihood function is derived:

\begin{align*}
    \ell\left(\beta, \sigma^{2}\right)=-\frac{1}{2}\left(n \log \sigma^{2}+\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\boldsymbol{x}_{i}^{T} \boldsymbol{\beta}\right)^{2}\right)
\end{align*}

$\beta$ is fixed at $\hat{\beta}$, and $\ell$ is minimised giving:

\begin{align*}
  \hat{\sigma^2}=\frac{1}{n}\sum_{i=1}{n}(y_i\boldsymbol{x}_i^T\hat{\beta})^2  
\end{align*}

If $n$ is not large, it's normalised by $n-p$ instead of $n$ for unbiasedness.

\subsubsection{Confidence and Prediction intervals}

When parameters are estimated for a model, confidence intervals are calculated for the coefficients, and prediction intervals are calculated for individual values.

Prediction intervals for $\hat{Y}$ are always wider than the confidence interval for $\hat{f}(x)$.

\subsection{Building models}

\begin{enumerate}
    \item Choose features for model
    \item Scatter plot each feature
    \item Consider feature transformations and interactions
    \item Fit the model
    \item Check and scrutinize the model
\end{enumerate}

\subsubsection{Testing models}

\begin{itemize}
    \item Test if coefficients are zero - feature has no impact if it it's coefficient is
    \item Use F-test and AIC/BIC to compare to other models with other features
\end{itemize}

\subsubsection{Model selection}

\begin{itemize}
    \item \textbf{Forward selection:} Start by including no/few variables
    \item \textbf{Backward selection:} Start by including all/many variables
    \item \textbf{Alternating:} Alternate between forward and backward selection
\end{itemize}

\pagebreak

\end{document}
