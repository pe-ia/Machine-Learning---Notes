\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}

\begin{document}

\tableofcontents

\pagebreak

\section{Introduction to Machine Learning}

\begin{itemize}
    \item Machine learning algorithms are statistical algorithms that can learn from data and generalize to unseen data, thus performing tasks without explicit instructions.
    \item In supervised methods we know the outcome Y for each input X; often, the task is to predict Y based on X.
    \item In unsupervised methods we have only the inputs X. Typical tasks are about finding structure in data.
    \begin{itemize}
        \item For example, it can be used to find "clusters".
    \end{itemize}
    \item X is often known as:
    \begin{itemize}
        \item Input
        \item Feature
        \item Predictor
        \item Covariate
        \item Independent variable
    \end{itemize}
    \item Y is often known as:
    \begin{itemize}
        \item Output
        \item Outcome
        \item Response
        \item Target variable
        \item Dependent variable
    \end{itemize}
    \item Supervised problems are often referred to as either:
    \begin{itemize}
        \item \textbf{Regression}: The outcome Y is quantitative (typically $\mathbb{R}$)s
        \item \textbf{Classification}: The outcome Y is categorical
    \end{itemize}
\end{itemize}

\pagebreak

\section{Chapter 3: Linear Regression}

\subsection{3.1 Simple Linear Regression}
Simple linear regression models the relationship between a single predictor \(X\) and a response \(Y\) using a linear function:
\[
Y = \beta_0 + \beta_1 X + \epsilon,
\]
where:
\begin{itemize}
    \item \(\beta_0\) is the intercept,
    \item \(\beta_1\) is the slope of the line,
    \item \(\epsilon\) is the error term, which is assumed to be normally distributed with mean 0.
\end{itemize}
The coefficients \(\beta_0\) and \(\beta_1\) are estimated using the least squares criterion, which minimizes the residual sum of squares (RSS):
\[
RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2.
\]

\textbf{Interpretation:} 
\begin{itemize}
    \item \(\beta_1\) represents the average change in the response \(Y\) associated with a one-unit change in the predictor \(X\).
    \item For example, in predicting sales based on TV advertising spend, \(\beta_1\) would indicate how much sales increase, on average, for each additional unit of advertising expenditure on TV.
\end{itemize}

\subsection{3.2 Multiple Linear Regression}
Multiple linear regression extends the simple linear regression model to include multiple predictors:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon.
\]
Here:
\begin{itemize}
    \item Each \(\beta_j\) represents the change in \(Y\) associated with a one-unit change in \(X_j\), holding all other predictors constant.
    \item The model can be written in matrix notation as:
    \[
    Y = X\beta + \epsilon,
    \]
    where \(Y\) is an \(n \times 1\) vector of responses, \(X\) is an \(n \times (p+1)\) matrix of predictors (including a column of ones for the intercept), \(\beta\) is a \((p+1) \times 1\) vector of coefficients, and \(\epsilon\) is an \(n \times 1\) vector of errors.
\end{itemize}

\textbf{Example: Advertising Data.} Consider a multiple regression model for predicting sales using TV, radio, and newspaper advertising budgets:
\[
\text{sales} = \beta_0 + \beta_1 \cdot \text{TV} + \beta_2 \cdot \text{radio} + \beta_3 \cdot \text{newspaper} + \epsilon.
\]
The estimated coefficients \(\beta_1, \beta_2,\) and \(\beta_3\) help to understand the individual contribution of each type of advertising while accounting for the other types.

\subsection{3.3 Other Considerations in the Regression Model}
\textbf{Qualitative Predictors:} 
Qualitative or categorical predictors can be incorporated into regression models by creating dummy variables. For example, if the predictor is a categorical variable 'region' with three levels (East, West, North), we can introduce two dummy variables:
\[
\text{region}_1 = 
\begin{cases} 
1 & \text{if East} \\
0 & \text{otherwise}
\end{cases}, \quad 
\text{region}_2 = 
\begin{cases} 
1 & \text{if West} \\
0 & \text{otherwise}
\end{cases}.
\]
Incorporating these into the regression model allows us to estimate different intercepts for each region.

\textbf{Interaction Terms:}
Interaction terms allow modeling situations where the effect of one predictor depends on another. For example, if we believe that the effect of TV advertising on sales depends on the level of radio advertising, we can include an interaction term:
\[
\text{sales} = \beta_0 + \beta_1 \cdot \text{TV} + \beta_2 \cdot \text{radio} + \beta_3 \cdot (\text{TV} \times \text{radio}) + \epsilon.
\]

\textbf{Extensions and Potential Problems:}
\begin{itemize}
    \item \textbf{Polynomial Regression:} Adds polynomial terms to the regression model to capture non-linear relationships.
    \item \textbf{Potential Issues:} Multicollinearity (high correlation among predictors), non-linearity of relationships, and outliers can affect model estimates.
\end{itemize}


\section{Bias and Variance}

Assuming $Y=f(X_0)+\varepsilon$, where:
\begin{itemize}
    \item $\mathbb{E}(\varepsilon)=0$
    \item $\varepsilon$ is independent of $X_0$
\end{itemize}
When fitting $\hat{f}$ with a dataset of $N$ observations ($X$,$Y$)
\begin{itemize}
    \item $N$ observations = \textit{training data}
    \item Algorithm for fitting $\hat{f}$ = \textit{learner}
    \item Applying learner to the training data = \textit{training}
\end{itemize}
Given training data and a chosen model framework, we want to minimize \textit{expected test MSE}: 
\begin{align*}
    \mathbb{E}[(Y_0-\hat{f}(X_0))^2]
\end{align*}
The expected test MSE at $x_0$ is:
\begin{align*}
    \mathbb{E}(Y_0-\hat{f}(x_0))=\mathbb{E}(\hat{f}(x_0)-\mathbb{E}(\hat{f}(x_0)))^2+(\mathbb{E}(\hat{f}(x_0))-f(x_0))^2+\text{Var}(\varepsilon)
\end{align*}
Where:
\begin{itemize}
    \item $\mathbb{E}(\hat{f}(x_0)-\mathbb{E}(\hat{f}(x_0)))^2$ is the Variance of $\hat{f}(x_0)$, the amount by which $\hat{f}$ would change if it were estimated with a different training data set
    \item $\mathbb{E}(\hat{f}(x_0))-f(x_0)$ is the Bias of $\hat{f}(X_0)$, the expected deviation of the model prediction from the true value - a.k.a., the inability to capture the true relationship
\end{itemize}
In the MSE, all 3 terms are non-negative, so any large term = large MSE; however, variance and bias are both \textit{reducible errors}.

\subsection{Validation sets}

If we wish to use MSE for model building - selecting features and tuning hyperparameters - we create a validation set, separate from training/testing set, for estimating the MSE.

\subsubsection{Leave-one-out crossvalidation (LOOCV)}

Create $n$ different partitions (where $n$ is amount of data points), with just one validation data point in each.
For each partition, $i$, the test MSE is $(y_i-\hat{y_i})^2$.
The LOOCV estimate for the test MSE is:
\begin{align*}
    \text{MSE} = \frac{1}{n}\sum_{i=n}^{n}(y_i-\hat{y_i})^2
\end{align*}
Downside: This is computationally expensive, requiring $n$ fits.

\subsubsection{$k$-fold crossvalidation}

Create $k$ different partitions ("folds"), in each $1/k$ of the data is for validation. Randomly shuffle data beforehand if it's ordered.

For each fold, compute $\text{MSE}_i$. Then, average:
\begin{align*}
    \frac{1}{k}\sum_{i=1}^{k}\text{MSE}_k
\end{align*}
Downside: Non-deterministic.

\section{Regression with many features}

Problem:
\begin{itemize}
    \item When features $p \approx n$, not enough data to estimate the regression curve well.
    \item When $p > n$, MLE doesn't exist.
\end{itemize}

Solutions:
\begin{itemize}
    \item Model selection
    \item Shrinkage (regularisation)
\end{itemize}

\subsection{Model selection}

As mentioned in 2.6.2, several methods:
\begin{itemize}
    \item \textbf{Best subset:} Self-explanitory, expensive
    \begin{itemize}
        \item Genetic algorithm can be used as heuristic
    \end{itemize}
    \item \textbf{Forward selection:} Start by including no/few variables
    \item \textbf{Backward selection:} Start by including all/many variables
    \item \textbf{Alternating:} Alternate between forward and backward selection
\end{itemize}

For comparing models, AIC, BIC, or test error can be used.

\subsubsection{AIC and BIC}

\begin{align*}
    \text{AIC}=-2\log\hat{L}+2p
\end{align*}
\begin{align*}
    \text{BIC}=-2\log\hat{L}+p \log n
\end{align*}
Where $p$ is the number of parameters, $n$ is the number of features, and d $\hat{L}$ is the maximised likelihood function. Smaller = better.

\subsubsection{Stepwise selection}

\textbf{Backwards:}
\begin{enumerate}
    \item Make initial model, with all/many features
    \item Iterative remove the least relevant feature (largest drop in AIC)
    \item Stop if removing a feature does not drop AIC significantly
\end{enumerate}

\textbf{Forwards:}
\begin{enumerate}
    \item Make initial model, with only intercept/few features
    \item Iterative add the most relevant feature (largest drop in AIC)
    \item Stop if adding a feature does not drop AIC significantly
\end{enumerate}

\subsubsection{Notes on automatic model selection}

\begin{itemize}
    \item One may wish to keep certain variables, even if they are not significant, if they are relevant in some other way, such as to the research question.
    \item One may wish to remove certain variables, even if they are significant, if they complicate interpretations.
\end{itemize}

\subsection{Shrinkage}

Shrinkage helps estimate coefficients $\beta$ in a constrained way, keeping them small. Doing so reduces coefficient variance and can improve the fit.

\subsubsection{Ridge regression}

Ridge regression minimizes the residual sum of squares, but adds a penalty term proportional to the squared magnitude of the coefficients:

\begin{align*}
    \hat{\beta}_\text{ridge} = \underset{\beta}{\mathrm{argmin}} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
\end{align*}

Where:
\begin{itemize}
    \item $\lambda \geq 0$ is a tuning parameter that controls the strength of the penalty.
    \item A larger $\lambda$ leads to more shrinkage and smaller coefficients.
    \item Ridge regression shrinks coefficients towards zero but never sets them exactly to zero.
    \item $\text{argmin}$ (short for "argument of the minimum") finds the value of $\beta$ that minimizes the function inside the curly braces. In this case, it finds the coefficients that minimize the sum of squared errors plus the penalty term.
\end{itemize}

\subsubsection{Lasso}

Lasso (Least Absolute Shrinkage and Selection Operator) regression also minimizes the residual sum of squares, but it adds a penalty term proportional to the absolute value of the coefficients:

\begin{align*}
    \hat{\beta}^\text{lasso} = \underset{\beta}{\mathrm{argmin}} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
\end{align*}

Where:
\begin{itemize}
    \item $\lambda \geq 0$ is a tuning parameter that controls the strength of the penalty.
    \item Lasso encourages sparsity, meaning that some coefficients are set exactly to zero for sufficiently large $\lambda$.
    \item Lasso can be used for both regularization and feature selection.
\end{itemize}

\subsubsection{Lasso vs Ridge}

\textbf{TLDR}: Ridge scales everything down, while Lasso also gets rid of small enough coefficients.

\begin{itemize}
    \item \textbf{Ridge:}
    \begin{itemize}
        \item Ridge regression applies an $L_2$ penalty, which shrinks the coefficients continuously towards zero.
        \item It does not perform feature selection as it keeps all coefficients, though they may be very small.
    \end{itemize}
    
    \item \textbf{Lasso:}
    \begin{itemize}
        \item Lasso applies an $L_1$ penalty, which shrinks some coefficients to zero exactly.
        \item It performs both regularization and feature selection by setting some coefficients to zero.
    \end{itemize}
    
    \item \textbf{Choosing between them:}
    \begin{itemize}
        \item Lasso is preferred when you suspect many features are irrelevant and need automatic feature selection.
        \item Ridge is preferred when all features are potentially relevant but need regularization to prevent overfitting.
        \item Elastic net is a compromise between Ridge and Lasso, applying a combination of $L_1$ and $L_2$ penalties.
    \end{itemize}
\end{itemize}

\end{document}
